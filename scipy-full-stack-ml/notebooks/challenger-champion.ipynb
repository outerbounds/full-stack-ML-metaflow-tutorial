{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Results\n",
    "\n",
    "First, let's collect the performance of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow, namespace\n",
    "\n",
    "# these values are unique to your deployment!\n",
    "CHAMPION_MODEL_NAMESPACE = \"production:mfprj-xsfdb3gtsiboqyrd-0-vqsy\" # \"production:mfprj-xsfdb3gtsiboqyrd-0-vqsy\"\n",
    "CHALLENGER_MODEL_NAMESPACE = \"production:mfprj-cfyzlfzievjlmsf4-0-tbgz\" # \"production:mfprj-cfyzlfzievjlmsf4-0-tbgz\"\n",
    "\n",
    "best_score = -1; winner = None; winner_namespace = None\n",
    "for n in [CHAMPION_MODEL_NAMESPACE, CHALLENGER_MODEL_NAMESPACE]:\n",
    "    namespace(n)\n",
    "    run = Flow('TitanicSurvivalPredictor').latest_successful_run\n",
    "    acc_score = run.data.score\n",
    "    if acc_score > best_score:\n",
    "        best_score = acc_score\n",
    "        winner = run.data.model_type\n",
    "        winner_namespace = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The winner is the {} model, with accuracy of {}%. You can find the model in the flow deployed to the {} namespace.\".format(winner, round(100*best_score, 2), winner_namespace))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, instead of evaluating on a static validation set, you may want to send production data to each model to run an A/B test before fully replacing an incumbent model. \n",
    "\n",
    "The details of doing this at scale are beyond the scope of this course, but now you have all the infrastructural tools at your disposal - and you can always practice more advanced deployment strategies for free in your Metaflow sandbox!\n",
    "\n",
    "In the next lesson, we will point you in the direction of production A/B testing. Specifically, you will restart your model server if you turned it off, and start sending traffic to the challenger model you deployed and analyzed in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full-stack-metaflow-corise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
