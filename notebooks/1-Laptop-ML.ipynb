{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767f5035-57e9-4125-959f-0697b11803a3",
   "metadata": {},
   "source": [
    "# Lesson 1: Laptop Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7fa21e-0a75-4d76-83b0-909fbf1ce63e",
   "metadata": {},
   "source": [
    "## Learning objectives of this lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fd5ce-269b-4163-bd85-842cde51b93b",
   "metadata": {},
   "source": [
    "* A quick refresher on what machine learning is\n",
    "* Refresh ourselves as to some of the ways to do machine learning in Python\n",
    "* Build random forests with scikit-learn\n",
    "* Build boosted trees with xgboost\n",
    "* Build neural networks with keras and tensorflow\n",
    "\n",
    "The purpose of this refresher is (1) to set the scene and (2) so that we have some typical ML code that we can then productionize!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949c5aa-b5af-4000-93db-836c0fce396a",
   "metadata": {},
   "source": [
    "## What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa9499-0a04-47b6-88be-802e5c0366f2",
   "metadata": {},
   "source": [
    "Machine learning is the science and art of teaching computers to \"learn\" patterns from data. In some ways, we can consider it a subdiscipline of data science, which is often sliced into\n",
    "\n",
    "* Descriptive analytics (BI, classic analytics, dashboards),\n",
    "* Predictive analytics (machine learning), and\n",
    "* Prescriptive analytics (decision science).\n",
    "\n",
    "Machine learning itself is often sliced into\n",
    "\n",
    "* Supervised learning (predicting a label: classification, or a continuous variable),\n",
    "* Unsupervised learning (pattern recognition for unlabelled data, a paradigm being clustering),\n",
    "* Reinforcement learning, in which software agents are placed in constrained environments and given “rewards” and “punishments” based on their activity (AlphaGo Zero, self-driving cars). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf3f84-7698-4c4c-8dc1-1335231ea219",
   "metadata": {},
   "source": [
    "## Machine Learning: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538b92d-49cc-44f8-a000-f7b992940802",
   "metadata": {},
   "source": [
    "So we're now going to jump in and build our first machine learning model. It is the (now) famous Iris dataset, where each row consists of measurements of a flower and the target variable (the one you're trying to predict) is the species of flower. \n",
    "\n",
    "**On terminology:**\n",
    "\n",
    "- The **target variable** is the variable you are trying to predict;\n",
    "- Other variables are known as **features** (or **predictor variables**), the features that you're using to predict the target variable.\n",
    "\n",
    "**On practice and procedure:**\n",
    "\n",
    "To build machine learning models, you require two things:\n",
    "\n",
    "- **Training data** (which the algorithms learn from) and\n",
    "- An **evaluation metric**, such as accuracy.\n",
    "\n",
    "For more on these, check out Cassie Kozyrkov's wonderful articles [Forget the robots! Here’s how AI will get you](https://towardsdatascience.com/forget-the-robots-heres-how-ai-will-get-you-b674c28d6a34) and [Machine learning — Is the emperor wearing clothes?](https://medium.com/@kozyrkov/machine-learning-is-the-emperor-wearing-clothes-928fe406fe09).\n",
    "\n",
    "After training your algorithm on your training data, you can use it to make predictions on a _labelled_ **holdout** (or **test**) set and compare those predictions with the known labels to compute how well it performs.\n",
    "\n",
    "You can also use a technique called **(k-fold) cross validation**, where you train and test several times using different holdout sets and compute the relevant accuracies (see more [here](https://en.wikipedia.org/wiki/Cross-validation_(statistics))). Image from Wikipedia:\n",
    "\n",
    "![flow0](../img/cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21026c7e-385d-41cb-9d4e-07dbdcce86a5",
   "metadata": {},
   "source": [
    "Also note that the ML ingredients of *training data* and *evaluation* metric can introduce all type of biases and other problems into your ML algorithms, for example:\n",
    "\n",
    "* If your training data is biased, your model more than likely will be;\n",
    "* If you optimize solely for accuracy, what happens to groups that are under-represented in your training data?\n",
    "\n",
    "The latter challenge follows from the broader class of problems we face when optimizing anything, as detailed by Rachel Thomas in [\"The problem with metrics is a big problem for AI\"](https://www.fast.ai/2019/09/24/metrics/):\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">The problem with metrics is a big problem for AI<br>- Most AI approaches optimize metrics<br>- Any metric is just a proxy<br>- Metrics can, and will, be gamed<br>- Metrics overemphasize short-term concerns<br>- Online metrics are gathered in highly addictive environment</a></p>&mdash; Rachel Thomas (@math_rachel) <a href=\"https://twitter.com/math_rachel/status/1176606580264951810?ref_src=twsrc%5Etfw\">September 24, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53389bf1-0106-4720-958f-bb24f3f7de29",
   "metadata": {},
   "source": [
    "### Typical Machine Learning code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9925b-c119-47d2-8689-1d4bb91a3e50",
   "metadata": {},
   "source": [
    "We'll now show how to build some typical ML models in Python for\n",
    "\n",
    "* random forests,\n",
    "* boosted trees, and\n",
    "* neural networks (deep learning).\n",
    "\n",
    "The intention is not to be exhaustive but rather to show typical code for the 3 most practical types of models that you will write. We won't go into the details of all of these models but we will link to relevant resources so you can explore to your heart's content!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df539285-2961-44af-b913-e4fd8c93f05e",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275a72d-9c02-4e16-ba74-2291e66b4bf8",
   "metadata": {},
   "source": [
    "[Random forests](https://scikit-learn.org/stable/modules/ensemble.html#forest) are both powerful and commonly use ML algorithms. In the following, we\n",
    "\n",
    "* Load our dataset,\n",
    "* Instantiate three models: decision tree, random forest, and extra trees classifier, and\n",
    "* Perform cross-validation for each model\n",
    "\n",
    "Note that we're building more than just random forests here but we couldn't help ourselves as scikit-learn makes it so easy! These examples are from the [scikit-learn documentation](https://scikit-learn.org/stable/modules/ensemble.html#forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9decbcc8-ef14-4081-863e-766ea206052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 0.96666667 0.9        0.96666667 1.        ]\n",
      "[0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
      "[0.96666667 0.96666667 0.93333333 0.9        1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf_dt = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "scores_dt = cross_val_score(clf_dt, X, y, cv=5)\n",
    "print(scores_dt)\n",
    "\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores_rf = cross_val_score(clf_rf, X, y, cv=5)\n",
    "print(scores_rf)\n",
    "\n",
    "\n",
    "clf_et = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores_et = cross_val_score(clf_et, X, y, cv=5)\n",
    "print(scores_et)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4636c5c-511f-4de1-9218-45d8578019d3",
   "metadata": {},
   "source": [
    "### Boosted trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49546aba-ded5-421f-a8b7-21827a995371",
   "metadata": {},
   "source": [
    "[Boosted trees](https://en.wikipedia.org/wiki/Gradient_boosting) are similar to random forests, in that they're both ensembles of decision trees. They are built differently, however. You can read [here](https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80) about the differences.\n",
    "\n",
    "We'll use XGBoost, which is a popular package for boosted trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04f5ea0-db93-4589-97ee-86f23da1a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28583017 0.9239239  0.28583017 ... 0.9239239  0.05169873 0.9239239 ]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix('../data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('../data/agaricus.txt.test')\n",
    "# specify parameters\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic', 'eval_metric':'logloss'}\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82c397-d206-440f-a85e-ebc8a53fbe87",
   "metadata": {},
   "source": [
    "### Neural nets and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed7261-7263-44a0-af9a-cbb54533869e",
   "metadata": {},
   "source": [
    "The third type of algorithm we'll now build is a neural network (also known as deep learning). These are:\n",
    "\n",
    "\n",
    "- ML models inspired by biological neural networks.\n",
    "- Performant for image classification, NLP, and more.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![flow0](../img/george.jpg)\n",
    "\n",
    "Image from [here](https://www.pnas.org/content/116/4/1074/tab-figures-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f57af1-9ce6-4ade-989e-a5b33195440a",
   "metadata": {},
   "source": [
    "When making predictions with neural networks, we use a procedure called **forward propagation**. When training neural networks (that is, finding the parameters, called weights), we use a procedure called **backpropogation**. To put it another way,\n",
    "\n",
    "- **forward propagation** is for prediction (`.predict()`);\n",
    "- **backpropogation** is for training (`.fit()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af72baa-412d-4b5d-8a9b-49a9fcd635c1",
   "metadata": {},
   "source": [
    "\n",
    "The following is (somewhat) typical deep learning code. We're using Keras & TensorFlow (and the example is based on the [Keras documentation](https://keras.io/examples/vision/mnist_convnet/)) but you have many other options, such as PyTorch, fast.ai, JAX, and/or PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2718e0ab-b848-4a0d-af38-2d1e9fc0672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-08 14:42:09.047013: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b376cd-62a5-420e-947a-babfa55d1261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-08 14:42:12.520507: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 12s 27ms/step - loss: 0.7665 - accuracy: 0.7605 - val_loss: 0.0798 - val_accuracy: 0.9790\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.1190 - accuracy: 0.9634 - val_loss: 0.0568 - val_accuracy: 0.9835\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0872 - accuracy: 0.9731 - val_loss: 0.0480 - val_accuracy: 0.9865\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0683 - accuracy: 0.9780 - val_loss: 0.0426 - val_accuracy: 0.9882\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0629 - accuracy: 0.9816 - val_loss: 0.0380 - val_accuracy: 0.9900\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0546 - accuracy: 0.9835 - val_loss: 0.0387 - val_accuracy: 0.9895\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0516 - accuracy: 0.9839 - val_loss: 0.0349 - val_accuracy: 0.9902\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0481 - accuracy: 0.9839 - val_loss: 0.0335 - val_accuracy: 0.9905\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0447 - accuracy: 0.9853 - val_loss: 0.0316 - val_accuracy: 0.9918\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0398 - accuracy: 0.9871 - val_loss: 0.0314 - val_accuracy: 0.9907\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.0325 - val_accuracy: 0.9913\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0358 - accuracy: 0.9890 - val_loss: 0.0315 - val_accuracy: 0.9908\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0351 - accuracy: 0.9879 - val_loss: 0.0294 - val_accuracy: 0.9917\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0317 - accuracy: 0.9901 - val_loss: 0.0319 - val_accuracy: 0.9905\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 11s 26ms/step - loss: 0.0322 - accuracy: 0.9896 - val_loss: 0.0308 - val_accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa9b4ca22e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fba1ae3-8e0d-498a-9b6a-1e232b00fd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.027709564194083214\n",
      "Test accuracy: 0.9907000064849854\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c21dec-0d56-4220-bf7c-f673c9077af2",
   "metadata": {},
   "source": [
    "### Lesson Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d5b1b-1ffa-41de-99ab-895e1b7ce579",
   "metadata": {},
   "source": [
    "In this lesson, we covered the following:\n",
    "\n",
    "* A quick refresher on what machine learning is\n",
    "* Refreshing ourselves as to some of the ways to do machine learning in Python\n",
    "* Building random forests with scikit-learn\n",
    "* Building boosted trees with xgboost\n",
    "* Building neural networks with keras and tensorflow\n",
    "\n",
    "In the next lesson, we'll take these machine learning workflows and see what it means to productionize them!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full-stack-metaflow] *",
   "language": "python",
   "name": "conda-env-full-stack-metaflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
