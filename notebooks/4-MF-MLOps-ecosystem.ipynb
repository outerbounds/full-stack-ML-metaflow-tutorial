{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78d30c4-4b6c-4ed2-a61a-6e434f6db9d3",
   "metadata": {},
   "source": [
    "# Lesson 4: Metaflow and the MLOps ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74dc9fa-cb55-490d-8749-4eb009385274",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da9868-d530-4e06-9b78-5325482ad519",
   "metadata": {},
   "source": [
    "* Incorporate other tools from the MLOps ecosystem into your ML workflows, including\n",
    "    - Experiment tracking,\n",
    "    - Data validation, and\n",
    "    - Deploying your model to an endpoint.\n",
    "    \n",
    "Note that lessons 1-3 that we have just covered get you far! As your projects mature, the more advanced topics in Lesson 4 become relevant. In this lesson, we'll demo some more advanced things that are possible with modern tooling. Setting all this up requires some effort, so take this more as an inspirational tour rather than a step-by-step tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80abe8c6-2bf5-489f-820a-bc4dd012976b",
   "metadata": {},
   "source": [
    "## Interoperability as a Foundational Part of Full-Stack ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aae4cc-32d8-4c93-8c4c-3096557e6656",
   "metadata": {},
   "source": [
    "_Human-centricity_ is a foundational principle of Metaflow. As a result, MF strives to be interoperable and compatible with all the other ML tools that you already use (and ones you may want to use!). In this lesson, we'll show how to incorporate 3 _types of tools_, those for \n",
    "* experiment tracking,\n",
    "* data validation, and\n",
    "* deployment.\n",
    "\n",
    "We'll be using [Weights & Biases](https://wandb.ai/site) for experiment tracking, [Great Expectations](https://greatexpectations.io/) for data validation, and [Amazon SageMaker](https://aws.amazon.com/pm/sagemaker/) for deployment, but keep in mind that Metaflow is agnostic with respect to the other tools you use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96086037-dcff-4192-b524-6b1095a6e646",
   "metadata": {},
   "source": [
    "![flow0](../img/recsys_flow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40bf3b-f2fc-41e1-8a00-cac95ab52f56",
   "metadata": {},
   "source": [
    "This figure is from the wonderful repo [You Don't Need a Bigger Boat](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e83c9a-80fd-4ff3-87c4-af6d1edb4ab9",
   "metadata": {},
   "source": [
    " Let's jump in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332dddb-cea3-4cb0-ac41-ef8af6db24fc",
   "metadata": {},
   "source": [
    "## Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3755bb-e811-4e66-8b8e-25cf7f8d3b27",
   "metadata": {},
   "source": [
    "Experiment tracking is a way to keep track of all the model runs you try, along with those models in production. In the following, we use [Weights and Biases](https://wandb.ai/site) but there are other options, such as [Neptune.ai](https://neptune.ai/) and [Comet](https://www.comet.ml/site/). To reproduce the following, you will need to create a free Weights and Biases account. \n",
    "\n",
    "Note that  Metaflow tracks all experiments for you automatically, as we saw in previous lessons, so you donâ€™t need a separate tool for that. However, a tool like W&B is convenient for many things, such as comparing results of a run with an easy-to-use UI out of the box and you can use it easily with Metaflow.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a57436-3384-4d2d-bebc-6c5280c3e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/ecosystem/rf_flow_monitor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/ecosystem/rf_flow_monitor.py\n",
    "from metaflow import FlowSpec, step, card\n",
    "import json\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.labels = self.iris['target_names']\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "        self.next(self.rf_model)\n",
    "        \n",
    "\n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.clf.predict(self.X_test)\n",
    "        self.y_probs = self.clf.predict_proba(self.X_test)\n",
    "        self.next(self.monitor)\n",
    "        \n",
    "    @step\n",
    "    def monitor(self):\n",
    "        \"\"\"\n",
    "        plot some things using an experiment tracker\n",
    "        \n",
    "        \"\"\"\n",
    "        import wandb\n",
    "        # edit the following with your username, project name, etc ...\n",
    "        wandb.init(project=\"mf-rf-wandb\", entity=\"hugobowne\", name=\"mf-tutorial-iris\")\n",
    "\n",
    "        wandb.sklearn.plot_class_proportions(self.y_train, self.y_test, self.labels)\n",
    "        wandb.sklearn.plot_learning_curve(self.clf, self.X_train, self.y_train)\n",
    "        wandb.sklearn.plot_roc(self.y_test, self.y_probs, self.labels)\n",
    "        wandb.sklearn.plot_precision_recall(self.y_test, self.y_probs, self.labels)\n",
    "        wandb.sklearn.plot_feature_importances(self.clf)\n",
    "\n",
    "        wandb.sklearn.plot_classifier(self.clf, \n",
    "                              self.X_train, self.X_test, \n",
    "                              self.y_train, self.y_test, \n",
    "                              self.y_pred, self.y_probs, \n",
    "                              self.labels, \n",
    "                              is_binary=True, \n",
    "                              model_name='RandomForest')\n",
    "\n",
    "        wandb.finish()\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd347c-d95f-4a6e-92be-e803c55b5e96",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/ecosystem/rf_flow_monitor.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17df9c0-e664-42ca-b1b1-e14ecc44b6ac",
   "metadata": {},
   "source": [
    "Check out the monitoring dashboards you built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6ed88c-1cf1-491b-8c85-c1dfe142a294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/hugobowne/mf-rf-wandb/workspace?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7fb661f44eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "%wandb hugobowne/mf-rf-wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ddef8-2af8-4b51-9d1a-cbdff17cdce3",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ff703-f242-462e-b5de-4f9c6051504d",
   "metadata": {},
   "source": [
    "Data validation is an essential and underappreciated part of machine learning and data science, more generally! The basic idea is that, if you're expecting your data to have certain characteristics, you need to make sure it actually does and you need to automate this in production.\n",
    "\n",
    "For example, you may expect \n",
    "\n",
    "* your data to have particular features or\n",
    "* your features to be in certain ranges.\n",
    "\n",
    "There are many ways to do this, including using [pytest](https://ericmjl.github.io/data-testing-tutorial/3-pytest-introduction/). Here we'll use the open source framework [Great Expectations](https://greatexpectations.io/). We've already defined what \"expectations\" we have of our data, which we'll go through, when we run our flow below. The core of our data validation is contained in this type of step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d22101-5fea-4871-aff6-ff0e93f09b1a",
   "metadata": {},
   "source": [
    "```\n",
    "@step\n",
    "def data_validation(self):\n",
    "    \"\"\"\n",
    "    Perform data validation with great_expectations\n",
    "    \"\"\"\n",
    "    from data_validation import validate_data\n",
    "\n",
    "    validate_data(current.run_id, current.flow_name, self.data_paths)\n",
    "\n",
    "    self.next(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b40f81-083a-49c0-ae96-e02cf7876f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/ecosystem/iris_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/ecosystem/iris_validate.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "import json\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.labels = self.iris['target_names']\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "        self.next(self.data_validation)\n",
    "        \n",
    "\n",
    "\n",
    "    @step\n",
    "    def data_validation(self):\n",
    "        \"\"\"\n",
    "        Perform data validation with great_expectations\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        from ruamel import yaml\n",
    "        import great_expectations as ge\n",
    "        from great_expectations.core.batch import RuntimeBatchRequest\n",
    "\n",
    "        context = ge.get_context()\n",
    "\n",
    "        \n",
    "        from sklearn import datasets\n",
    "        iris = datasets.load_iris()\n",
    "        df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "        df[\"target\"] = iris['target']\n",
    "        df[\"petal length (cm)\"][0] = -1\n",
    "\n",
    "        # configuration for data validation checkpoint\n",
    "        checkpoint_config = {\n",
    "            \"name\": \"flowers-test-flow-checkpoint\",\n",
    "            \"config_version\": 1,\n",
    "            \"class_name\": \"SimpleCheckpoint\",\n",
    "            \"run_name_template\": \"%Y%m%d-%H%M%S-flower-power\",\n",
    "            \"validations\": [\n",
    "                {\n",
    "                    \"batch_request\": {\n",
    "                        \"datasource_name\": \"flowers\",\n",
    "                        \"data_connector_name\": \"default_runtime_data_connector_name\",\n",
    "                        \"data_asset_name\": \"iris\",\n",
    "                    },\n",
    "                    \"expectation_suite_name\": \"flowers-testing-suite\",\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "        context.add_checkpoint(**checkpoint_config)\n",
    "\n",
    "        # results of data validation\n",
    "        # then build and view docs\n",
    "        results = context.run_checkpoint(\n",
    "            checkpoint_name=\"flowers-test-flow-checkpoint\",\n",
    "            batch_request={\n",
    "                \"runtime_parameters\": {\"batch_data\": df},\n",
    "                \"batch_identifiers\": {\n",
    "                    \"default_identifier_name\": \"<YOUR MEANINGFUL IDENTIFIER>\"\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        context.build_data_docs()\n",
    "        context.open_data_docs()\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d250171-5a6c-42eb-8ef7-50ffaddb18f4",
   "metadata": {},
   "source": [
    "Execute the following to run the flow with data validation:\n",
    "\n",
    "```bash\n",
    "python flows/ecosystem/iris_validate.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ef640-ef2d-4b43-ae59-7e4d5faa44e2",
   "metadata": {},
   "source": [
    "## Combination station: data validation + experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fad893-0929-467c-814a-d5fbe60bdba2",
   "metadata": {},
   "source": [
    "Let's now combine the above into a single flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0fed65c-a518-46d5-98c1-b91de4b0feb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/ecosystem/rf_flow_monitor_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/ecosystem/rf_flow_monitor_validate.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "import json\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.labels = self.iris['target_names']\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "        self.next(self.data_validation)\n",
    "        \n",
    "\n",
    "    @step\n",
    "    def data_validation(self):\n",
    "        \"\"\"\n",
    "        Perform data validation with great_expectations\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        from ruamel import yaml\n",
    "        import great_expectations as ge\n",
    "        from great_expectations.core.batch import RuntimeBatchRequest\n",
    "\n",
    "        context = ge.get_context()\n",
    "\n",
    "        \n",
    "        from sklearn import datasets\n",
    "        iris = datasets.load_iris()\n",
    "        df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "        df[\"target\"] = iris['target']\n",
    "        # df[\"sepal length (cm)\"][0] = -1\n",
    "\n",
    "\n",
    "        checkpoint_config = {\n",
    "            \"name\": \"flowers-test-flow-checkpoint\",\n",
    "            \"config_version\": 1,\n",
    "            \"class_name\": \"SimpleCheckpoint\",\n",
    "            \"run_name_template\": \"%Y%m%d-%H%M%S-flower-power\",\n",
    "            \"validations\": [\n",
    "                {\n",
    "                    \"batch_request\": {\n",
    "                        \"datasource_name\": \"flowers\",\n",
    "                        \"data_connector_name\": \"default_runtime_data_connector_name\",\n",
    "                        \"data_asset_name\": \"iris\",\n",
    "                    },\n",
    "                    \"expectation_suite_name\": \"flowers-testing-suite\",\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "        context.add_checkpoint(**checkpoint_config)\n",
    "\n",
    "\n",
    "        results = context.run_checkpoint(\n",
    "            checkpoint_name=\"flowers-test-flow-checkpoint\",\n",
    "            batch_request={\n",
    "                \"runtime_parameters\": {\"batch_data\": df},\n",
    "                \"batch_identifiers\": {\n",
    "                    \"default_identifier_name\": \"<YOUR MEANINGFUL IDENTIFIER>\"\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        context.build_data_docs()\n",
    "        context.open_data_docs()\n",
    "\n",
    "        self.next(self.rf_model)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.clf.predict(self.X_test)\n",
    "        self.y_probs = self.clf.predict_proba(self.X_test)\n",
    "        self.next(self.monitor)\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    @step\n",
    "    def monitor(self):\n",
    "        \"\"\"\n",
    "        plot some things using an experiment tracker\n",
    "        \n",
    "        \"\"\"\n",
    "        import wandb\n",
    "        wandb.init(project=\"mf-rf-wandb\", entity=\"hugobowne\", name=\"mf-tutorial-iris\")\n",
    "\n",
    "        wandb.sklearn.plot_class_proportions(self.y_train, self.y_test, self.labels)\n",
    "        wandb.sklearn.plot_learning_curve(self.clf, self.X_train, self.y_train)\n",
    "        wandb.sklearn.plot_roc(self.y_test, self.y_probs, self.labels)\n",
    "        wandb.sklearn.plot_precision_recall(self.y_test, self.y_probs, self.labels)\n",
    "        wandb.sklearn.plot_feature_importances(self.clf)\n",
    "\n",
    "        wandb.sklearn.plot_classifier(self.clf, \n",
    "                              self.X_train, self.X_test, \n",
    "                              self.y_train, self.y_test, \n",
    "                              self.y_pred, self.y_probs, \n",
    "                              self.labels, \n",
    "                              is_binary=True, \n",
    "                              model_name='RandomForest')\n",
    "\n",
    "        wandb.finish()\n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f572c50-f37b-4b04-9696-3d81e9172c70",
   "metadata": {},
   "source": [
    "Execute the above with the following\n",
    "\n",
    "```bash\n",
    "python flows/ecosystem/rf_flow_monitor_validate.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039eefda-e3e5-4845-bda8-4d9823162c5a",
   "metadata": {},
   "source": [
    "We can check out our experiment tracking once again with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abfb90d-27ba-4b2e-ab5b-bdb7f0cb02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# %wandb hugobowne/mf-rf-wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73070de-927d-499c-830e-2f876bed7d7c",
   "metadata": {},
   "source": [
    "## Deploying your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30206e-246e-42aa-8bff-2d90513e9afd",
   "metadata": {},
   "source": [
    "Now we get to deploy our model that we can ping for predictions from anywhere around the globe: wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d629100-86bf-4984-bb4c-379b297061e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤¯\n"
     ]
    }
   ],
   "source": [
    "print('\\U0001F92F')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144e6a7-8fe4-4771-8d57-dc97b6fb2f74",
   "metadata": {},
   "source": [
    "To do this you'll need to have the correct permissions set up on Amazon Sagemaker. You can find out how to get set up with Sagemaker [here](https://docs.aws.amazon.com/sagemaker/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e29287-a4e4-4e61-88c1-fdd7544c0bb6",
   "metadata": {},
   "source": [
    "Now a few words on deplying to an endpoint:\n",
    "- It is not the only way to deploy ML to production. For example, batch predictions are easier to structure as a workflow and you donâ€™t need endpoints for that, just regular workflows.\n",
    "- However, when integrating with other services, say, a product UI, you need a service that other services can call. This is where a system like Sagemaker hosting comes in handy.\n",
    "- Sagemaker Hosting is just one option amongst others - you could also use an open-source project called Seldon - or even build your own simple service with Pythonâ€™s Flask project, for example, but Sagemaker is conveniently hosted by AWS so we donâ€™t have to worry about infrastructure, at least after you have managed to configure Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522dbcdd-b77d-40b9-91ff-41703352bbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/ecosystem/RF-deploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/ecosystem/RF-deploy.py\n",
    "\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card, S3, environment\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('my.env')\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.labels = self.iris['target_names']\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "        self.next(self.rf_model)\n",
    "        \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.clf.predict(self.X_test)\n",
    "        self.y_probs = self.clf.predict_proba(self.X_test)\n",
    "        self.next(self.deploy)\n",
    "\n",
    "    @step\n",
    "    def deploy(self):\n",
    "        \"\"\"\n",
    "        Use SageMaker to deploy the model as a stand-alone, PaaS endpoint.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import time\n",
    "        import joblib\n",
    "        import shutil\n",
    "        import tarfile\n",
    "        from sagemaker.sklearn import SKLearnModel\n",
    "        \n",
    "        ROLE = os.getenv('ROLE')\n",
    "        CODE_LOCATION = os.getenv('CODE_LOCATION')\n",
    "\n",
    "\n",
    "        model_name = \"model\"\n",
    "        local_tar_name = \"model.tar.gz\"\n",
    "\n",
    "\n",
    "        os.makedirs(model_name, exist_ok=True)\n",
    "        # save model to local folder\n",
    "        joblib.dump(self.clf, \"{}/{}.joblib\".format(model_name, model_name))\n",
    "        # save model as tar.gz\n",
    "        with tarfile.open(local_tar_name, mode=\"w:gz\") as _tar:\n",
    "            _tar.add(model_name, recursive=True)\n",
    "        # save model onto S3\n",
    "        with S3(run=self) as s3:\n",
    "            with open(local_tar_name, \"rb\") as in_file:\n",
    "                data = in_file.read()\n",
    "                self.model_s3_path = s3.put(local_tar_name, data)\n",
    "                #print('Model saved at {}'.format(self.model_s3_path))\n",
    "        # remove local model folder and tar\n",
    "        shutil.rmtree(model_name)\n",
    "        os.remove(local_tar_name)\n",
    "        # initialize SageMaker SKLearn Model\n",
    "        sklearn_model = SKLearnModel(model_data=self.model_s3_path,\n",
    "                                     role=ROLE,\n",
    "                                     entry_point='flows/ecosystem/sm_entry_point.py',\n",
    "                                     framework_version='0.23-1',\n",
    "                                     code_location=CODE_LOCATION)\n",
    "        endpoint_name = 'HBA-RF-endpoint-{}'.format(int(round(time.time() * 1000)))\n",
    "        print(\"\\n\\n================\\nEndpoint name is: {}\\n\\n\".format(endpoint_name))\n",
    "        # deploy model\n",
    "        predictor = sklearn_model.deploy(instance_type='ml.c5.2xlarge',\n",
    "                                         initial_instance_count=1,\n",
    "                                         endpoint_name=endpoint_name)\n",
    "        # prepare a test input and check response\n",
    "        test_input = self.X\n",
    "        result = predictor.predict(test_input)\n",
    "        print(result)\n",
    "        \n",
    "        self.next(self.end)\n",
    "    \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ef123-ded7-433a-a443-052334fd675c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Execute the above with\n",
    "```bash\n",
    "python flows/ecosystem/RF-deploy.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ccece-8202-472b-8f58-f9c83d0a1268",
   "metadata": {},
   "source": [
    "We can also test pinging the endpoint with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c79fd6-0ef1-4358-a734-d8988df2b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data']\n",
    "\n",
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name='us-west-2')\n",
    "\n",
    "# The name of the endpoint. The name must be unique within an AWS Region in your AWS account. \n",
    "\n",
    "endpoint_name='HBA-RF-endpoint-1649656108929'\n",
    "\n",
    "\n",
    "# csv serialization\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=pd.DataFrame(X).to_csv(header=False, index=False),\n",
    "    ContentType=\"text/csv\",\n",
    ")\n",
    "\n",
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca081b-a932-4a00-861e-05d40b980d29",
   "metadata": {},
   "source": [
    "**Exercise for the avid reader:** Combine all the above into a flow that includes\n",
    "* data validation,\n",
    "* experiment tracking, and\n",
    "* deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb80da3-57ad-47c6-9fb7-4516b44260e0",
   "metadata": {},
   "source": [
    "## Lesson Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada31bdb-d2fe-4c42-8a9e-14b1d0201f00",
   "metadata": {},
   "source": [
    "In this lesson, you have seen the power of interoperability in Metaflow. We have\n",
    "\n",
    "* Incorporated other tools from the MLOps ecosystem into your ML workflows, including\n",
    "    - Experiment tracking,\n",
    "    - Data validation, and\n",
    "    - Deploying your model to an endpoint.\n",
    "    \n",
    "And yet this is just the tip of the iceberg! To explore more and discuss this quickly evolving space, come chat with us on our [community slack](http://slack.outerbounds.co)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e0c8bf-b93e-4562-b826-bdefe4ff2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤Ÿ\n"
     ]
    }
   ],
   "source": [
    "print('\\U0001F91F')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full-stack-metaflow] *",
   "language": "python",
   "name": "conda-env-full-stack-metaflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
