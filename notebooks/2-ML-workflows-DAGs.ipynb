{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923b9a08-9b88-424d-9512-4fef84a51b4f",
   "metadata": {},
   "source": [
    "# Lesson 2: Machine Learning Workflows and DAGs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffacc8-f2c7-4a0c-9003-1fbdd3df2a89",
   "metadata": {},
   "source": [
    "## Learning objectives of this lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43465cf4-dc05-44b9-aab8-d95fc9ee644b",
   "metadata": {},
   "source": [
    "* Understand what the Modern Data Stack for Production ML looks like\n",
    "* Appreciate where the focus of scientists needs to be in this stack\n",
    "* Write machine learning flows in Metaflow for \n",
    "    - Random forests\n",
    "    - Boosted trees\n",
    "    - Neural networks\n",
    "* Train models in parallel and choose most performant one using Metaflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab25a56-cc11-431d-8f5a-5c69c5f5a768",
   "metadata": {},
   "source": [
    "This lesson will focus on building local machine learning workflows using Metaflow, although the high-level concepts taught will be applicable to any workflow orchestrator. Attendees will get a feel for writing flows and DAGs to define the steps in their workflows. We’ll also use DAG cards to visualize our ML workflows. This lesson will be local computation and in the next lesson, we’ll burst to the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611f465-a272-434b-8331-2158cf8e9870",
   "metadata": {},
   "source": [
    "In this section, we take the machine learning scripts from the previous lesson and turn them into flows. Currently, in the spirit of not introducing more tools, we'll write our flows in notebook cells and we'll execute them using the command line in Jupyter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f45945-20ee-4541-a8e7-15f4e9bc368f",
   "metadata": {},
   "source": [
    "## The Modern Data Stack for Production ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef009c-4e06-4078-8a69-1b175f7f6e5d",
   "metadata": {},
   "source": [
    "There are many layers to the modern ML stack:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dedc06-05f9-4ad5-bfb1-cd524664ff89",
   "metadata": {},
   "source": [
    "![flow0](../img/modern-ML-stack.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1bcc2-b599-48c3-91b8-f13b6606c743",
   "metadata": {},
   "source": [
    "Although they are all incredibly important, data scientists should NOT be expected to be experts at all of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9806d-3e05-46d4-9b76-b707cad73198",
   "metadata": {},
   "source": [
    "![flow0](../img/data-triangle.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293f9fe-4d23-4593-ad69-2c563216d279",
   "metadata": {},
   "source": [
    "Let's have a look at some of what data scientists do to interact with the infrastructural layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54a906-aeb2-4a96-b949-2b035a8faf50",
   "metadata": {},
   "source": [
    "![flow0](../img/ds-workflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba509ed-e64e-4940-bc11-f5b41a75c017",
   "metadata": {},
   "source": [
    "And let's not forget that this is an iterative process!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d09cc-addf-465c-85bf-881c0c1c82f8",
   "metadata": {},
   "source": [
    "![flow0](../img/data-science-iteration.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550b698-d7cd-426a-84e4-5ccce7466d72",
   "metadata": {},
   "source": [
    "## Orchestrating your Machine Learning workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa70b76-a718-4544-9ddb-62f977262242",
   "metadata": {},
   "source": [
    "The complexity of ML workflows can be become complicated very quickly. To avoid a jungle of data pipelines and models, we need organizational principles for our workflows and _directed acyclic graphs_ have become a standard for this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e7e33-cd2e-4834-9397-3d6fb03e9793",
   "metadata": {},
   "source": [
    "<img src=\"../img/flow_ex_0.png\" alt=\"DAG\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849a81d-20c8-4337-8050-b69f328cc9fe",
   "metadata": {},
   "source": [
    "DAGs can often be more complicated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a624d-6cd3-4dd7-b98a-d79439c9a7b7",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7685a-982d-41e9-91d2-1d947b958df7",
   "metadata": {},
   "source": [
    "## Crafting Metaflows for Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0ab17-f71d-4e35-8862-346db0b1bbdb",
   "metadata": {},
   "source": [
    "ML flows can be broken down into steps, such as:\n",
    "\n",
    "- importing data\n",
    "- processing, wrangling, and/or transforming the data\n",
    "- data validation\n",
    "- model configuration\n",
    "- model training, and\n",
    "- model deployment.\n",
    "\n",
    "The first flow we write will be a template showing these steps.\n",
    "\n",
    "As stated above, in the spirit of not introducing more tools, we'll write our flows in notebook cells and we'll execute them using the command line in Jupyter Lab. Having said that, we encourage everybody to get comfortable writing Python scripts and using the command line as much as possible! Notebooks are wonderful for many things but not necessarily for writing and executing ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25950f2-13e6-4f33-a882-8f928b952b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/flow_template.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/flow_template.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Template for writing Metaflows\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from metaflow import FlowSpec, step, current, card\n",
    "\n",
    "\n",
    "class Template_Flow(FlowSpec):\n",
    "    \"\"\"\n",
    "    Template for Metaflows.\n",
    "    You can choose which steps suit your workflow.\n",
    "    We have included the following common steps:\n",
    "    - Start\n",
    "    - Process data\n",
    "    - Data validation\n",
    "    - Model configuration\n",
    "    - Model training\n",
    "    - Model deployment\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start Step for a Flow;\n",
    "        \"\"\"\n",
    "        print(\"flow name: %s\" % current.flow_name)\n",
    "        print(\"run id: %s\" % current.run_id)\n",
    "        print(\"username: %s\" % current.username)\n",
    "\n",
    "        # Call next step in DAG with self.next(...)\n",
    "        self.next(self.process_raw_data)\n",
    "\n",
    "    @step\n",
    "    def process_raw_data(self):\n",
    "        \"\"\"\n",
    "        Read and process data\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll read in and process your data\")\n",
    "\n",
    "        self.next(self.data_validation)\n",
    "\n",
    "    @step\n",
    "    def data_validation(self):\n",
    "        \"\"\"\n",
    "        Perform data validation\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll write your data validation code\")\n",
    "\n",
    "        self.next(self.get_model_config)\n",
    "\n",
    "    @step\n",
    "    def get_model_config(self):\n",
    "        \"\"\"\n",
    "        Configure model + hyperparams\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll configure your model + hyperparameters\")\n",
    "        self.next(self.train_model)\n",
    "\n",
    "    @step\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train your model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll train your model\")\n",
    "\n",
    "        self.next(self.deploy)\n",
    "\n",
    "    @step\n",
    "    def deploy(self):\n",
    "        \"\"\"\n",
    "        Deploy model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll deploy your model\")\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        DAG is done! Congrats!\n",
    "        \"\"\"\n",
    "        print('DAG ended! Woohoo!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Template_Flow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed676e5-0600-4b88-a4c1-e4780793ca3a",
   "metadata": {},
   "source": [
    "You can now run this flow by executing the following from the CLI:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed4d4a-b9d5-4539-9468-27dde306635e",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/flow_template.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac9c19-486c-4ce8-93e1-baef6eab08db",
   "metadata": {},
   "source": [
    "We can also (cheekily) execute from this NB using the handy iPython magic shBang (!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eff4783-4962-4422-8654-bb3d40abb1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.0\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mTemplate_Flow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:hba\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:48.574 \u001b[0m\u001b[1mWorkflow starting (run-id 1649210688571219):\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:48.582 \u001b[0m\u001b[32m[1649210688571219/start/1 (pid 25064)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:49.140 \u001b[0m\u001b[32m[1649210688571219/start/1 (pid 25064)] \u001b[0m\u001b[22mflow name: Template_Flow\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:49.862 \u001b[0m\u001b[32m[1649210688571219/start/1 (pid 25064)] \u001b[0m\u001b[22mrun id: 1649210688571219\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:49.862 \u001b[0m\u001b[32m[1649210688571219/start/1 (pid 25064)] \u001b[0m\u001b[22musername: hba\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:49.864 \u001b[0m\u001b[32m[1649210688571219/start/1 (pid 25064)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:49.872 \u001b[0m\u001b[32m[1649210688571219/process_raw_data/2 (pid 25072)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:50.454 \u001b[0m\u001b[32m[1649210688571219/process_raw_data/2 (pid 25072)] \u001b[0m\u001b[22mIn this step, you'll read in and process your data\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:50.520 \u001b[0m\u001b[32m[1649210688571219/process_raw_data/2 (pid 25072)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:50.528 \u001b[0m\u001b[32m[1649210688571219/data_validation/3 (pid 25076)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:51.079 \u001b[0m\u001b[32m[1649210688571219/data_validation/3 (pid 25076)] \u001b[0m\u001b[22mIn this step, you'll write your data validation code\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:51.154 \u001b[0m\u001b[32m[1649210688571219/data_validation/3 (pid 25076)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:51.162 \u001b[0m\u001b[32m[1649210688571219/get_model_config/4 (pid 25080)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:51.717 \u001b[0m\u001b[32m[1649210688571219/get_model_config/4 (pid 25080)] \u001b[0m\u001b[22mIn this step, you'll configure your model + hyperparameters\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:51.783 \u001b[0m\u001b[32m[1649210688571219/get_model_config/4 (pid 25080)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:51.791 \u001b[0m\u001b[32m[1649210688571219/train_model/5 (pid 25084)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:52.386 \u001b[0m\u001b[32m[1649210688571219/train_model/5 (pid 25084)] \u001b[0m\u001b[22mIn this step, you'll train your model\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:52.454 \u001b[0m\u001b[32m[1649210688571219/train_model/5 (pid 25084)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:52.462 \u001b[0m\u001b[32m[1649210688571219/deploy/6 (pid 25089)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:53.007 \u001b[0m\u001b[32m[1649210688571219/deploy/6 (pid 25089)] \u001b[0m\u001b[22mIn this step, you'll deploy your model\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:53.071 \u001b[0m\u001b[32m[1649210688571219/deploy/6 (pid 25089)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:53.078 \u001b[0m\u001b[32m[1649210688571219/end/7 (pid 25093)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:53.619 \u001b[0m\u001b[32m[1649210688571219/end/7 (pid 25093)] \u001b[0m\u001b[22mDAG ended! Woohoo!\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:53.681 \u001b[0m\u001b[32m[1649210688571219/end/7 (pid 25093)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-04-06 12:04:53.682 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python ../flows/flow_template.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727715f2-98e2-461f-9841-e019c3a66dba",
   "metadata": {},
   "source": [
    "Success! But what are all these outputs? I'm glad that you asked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0f3cb-4faf-452d-8a81-e457f01c286d",
   "metadata": {},
   "source": [
    "![flow0](../img/mf_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594071a-8dbf-4e42-b488-f7d53cb54152",
   "metadata": {},
   "source": [
    "- **Timestamp** denotes when the line was output.\n",
    "- The information inside the square brackets identifies a **task**.\n",
    "- Every Metaflow run gets a unique ID, a **run ID**.\n",
    "- A run executes the steps in order. The step that is currently being executed is denoted by **step name**.\n",
    "- A step may spawn multiple tasks which are identified by a **task ID**.\n",
    "- The combination of a flow name, run ID, step name, and a task ID,uniquely identify a task in your Metaflow environment, amongst all runs of any flows. Here, the flow name is omitted since it is the same for all lines. We call this globally unique identifier a **pathspec**.\n",
    "- Each task is executed by a separate process in your operating system, identified by a **process ID** aka _pid_. You can use any operating system level monitoring tools such as top to monitor resource consumption of a task based on its process ID.\n",
    "- After the square bracket comes a **log message** that may be a message output by Metaflow itself, like “Task is starting” in this example, or a line output by your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783703ae-daa1-4126-97cb-afcbc2511cd8",
   "metadata": {},
   "source": [
    "### Metaflow cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495bbe1-9619-4910-b693-365ebfb840e0",
   "metadata": {},
   "source": [
    "We can use MF cards to visualize aspects of our flow. In this case, there's not much to check out but we **can** see the DAG by using the CLI as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04417d-4786-405c-845c-28348f8afed1",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/flow_template.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b756922-edf6-44d9-8994-e4414ceb290a",
   "metadata": {},
   "source": [
    "## Time to Write a ML Flow: Orchestrating our Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebaf4f-7e2c-4c1c-aa62-9a045c9275e7",
   "metadata": {},
   "source": [
    "In this section, we'll turn the random forest above into a flow. Recall our RF code from Lesson 1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f8503-5a93-4643-a378-347fd1637525",
   "metadata": {},
   "source": [
    "```\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores_rf = cross_val_score(clf_rf, X, y, cv=5)\n",
    "print(scores_rf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044535b1-c653-41e4-a2ef-94c51997a53b",
   "metadata": {},
   "source": [
    "In order to turn this into a (Metaflow) flow, you first need to decide what your steps are going to be. In this case, we’ll have distinct steps to \n",
    "\n",
    "* Load the data\n",
    "* Instantiate the model\n",
    "* Train the model and perform cross-validation\n",
    "\n",
    "In general, this involves some design choices on the user’s part and we have some general rules of thumb [here](https://docs.metaflow.org/metaflow/basics).\n",
    "\n",
    "\n",
    "To then write your flow, you\n",
    "\n",
    "* Import `FlowSpec` and `step` outside your flow\n",
    "* Include step-specific imports within each step\n",
    "* Assign any data structures you wish to pass between steps to self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a864d3-32ad-4841-a684-66a53c3c4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/rf_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/rf_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model)\n",
    "        \n",
    "\n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"ClassificationFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7703623-d4cb-4641-81f1-19ddb43c6fb0",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/rf_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53459547-9b72-4eec-944e-71db6517faa0",
   "metadata": {},
   "source": [
    "We can check out the Metaflow card:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643169f-154d-4cee-a855-1e02db08cf9c",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/local/rf_flow.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3503b3-a53d-4013-86be-e950ab54690e",
   "metadata": {},
   "source": [
    "## The Unreasonable Effective of Branching Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8d26a-99d2-48dc-8c41-f3e405e557e3",
   "metadata": {},
   "source": [
    "Often, you'll want to train several independent models and then choose the best one. As they're indepedendent, you can train them in parallel. For this, you can use the concept of branching, which is exemplified in this figure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e815da0-ee75-43c9-ba15-a4f25df7f133",
   "metadata": {},
   "source": [
    "<img src=\"../img/flow_ex_0.png\" alt=\"DAG\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde03f82-6d44-476c-99b2-1eb41e390d4f",
   "metadata": {},
   "source": [
    "We'll show how to do this in Metaflow by writing a flow that has random forests, decision trees, and extra trees classifiers, tries them all and chooses the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5545fc2d-dcc4-4a37-bc8b-37ed28f98ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/tree_branch_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/tree_branch_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "class ClassificationFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train multiple tree based methods\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model, self.xt_model, self.dt_model)\n",
    "    \n",
    "                \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def xt_model(self):\n",
    "        \"\"\"\n",
    "        build extra trees classifier\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "\n",
    "        self.clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def dt_model(self):\n",
    "        \"\"\"\n",
    "        build decision tree classifier\n",
    "        \"\"\"\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "            random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "\n",
    "        self.next(self.choose_model)\n",
    "                        \n",
    "    @step\n",
    "    def choose_model(self, inputs):\n",
    "        \"\"\"\n",
    "        find 'best' model\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        def score(inp):\n",
    "            return inp.clf,\\\n",
    "                   np.mean(inp.scores)\n",
    "\n",
    "            \n",
    "        self.results = sorted(map(score, inputs), key=lambda x: -x[1]) \n",
    "        self.model = self.results[0][0]\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print('Scores:')\n",
    "        print('\\n'.join('%s %f' % res for res in self.results))\n",
    "        print('Best model:')\n",
    "        print(self.model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ClassificationFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fc354-9ef2-4326-a913-f2299fd3d8b4",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/tree_branch_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498134b-2f69-4dd2-b604-6e9c6e44fd49",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02c584-0fdb-459c-82b6-bbac836b2525",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/local/tree_branch_flow.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd4a43-3ea3-43f5-9d2f-c96ee7d18bb3",
   "metadata": {},
   "source": [
    "## Boosted Trees Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b27a2-f26c-471d-8164-598ed7d4cfb0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the XGBoost example from Lesson 1 into a flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0b86aa-498d-474f-bde4-1e183b5dd7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/boosted_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/boosted_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "\n",
    "class BSTFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a boosted tree\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data & train model\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "        dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "        param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic','eval_metric':'logloss'}\n",
    "        num_round = 2\n",
    "        bst = xgb.train(param, dtrain, num_round)\n",
    "        bst.save_model(\"model.json\")\n",
    "        self.next(self.predict)\n",
    "        \n",
    "    @step\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        make predictions\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "\n",
    "        dtest = xgb.DMatrix('data/agaricus.txt.test')\n",
    "        # make prediction\n",
    "        bst = xgb.Booster()\n",
    "        bst.load_model(\"model.json\")\n",
    "        preds = bst.predict(dtest)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow, yo!\n",
    "        \"\"\"\n",
    "        print(\"BSTFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BSTFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1fce4-47cd-4c23-b770-7af10f785aee",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/boosted_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757332f-b475-4e42-b546-c97a75068fc9",
   "metadata": {},
   "source": [
    "## Your First Deep Learning Metaflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95df17-f187-4d4f-ad2e-894898f9b9b0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the deep learning example above into a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92adeacf-5396-42d0-bed2-32a8a9763d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../flows/local/NN_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../flows/local/NN_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "from taxi_modules import init, MODELS, MODEL_LIBRARIES\n",
    "import json\n",
    "\n",
    "\n",
    "class NNFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a NN\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        from tensorflow import keras\n",
    "\n",
    "        # the data, split between train and test sets\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.mnist.load_data()\n",
    "        self.next(self.wrangle)\n",
    "        \n",
    "    @step\n",
    "    def wrangle(self):\n",
    "        \"\"\"\n",
    "        massage data\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from tensorflow import keras\n",
    "        # Model / data parameters\n",
    "        self.num_classes = 10\n",
    "        self.input_shape = (28, 28, 1)\n",
    "\n",
    "        # Scale images to the [0, 1] range\n",
    "        self.x_train = self.x_train.astype(\"float32\") / 255\n",
    "        self.x_test = self.x_test.astype(\"float32\") / 255\n",
    "        # Make sure images have shape (28, 28, 1)\n",
    "        self.x_train = np.expand_dims(self.x_train, -1)\n",
    "        self.x_test = np.expand_dims(self.x_test, -1)\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = keras.utils.to_categorical(self.y_train, self.num_classes)\n",
    "        self.y_test = keras.utils.to_categorical(self.y_test, self.num_classes)\n",
    "        \n",
    "        self.next(self.build_model)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        build NN model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=self.input_shape),\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            tf.keras.models.save_model(model, f.name, save_format='h5')\n",
    "            self.model = f.read()\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import tensorflow as tf\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 15\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            f.write(self.model)\n",
    "            f.flush()\n",
    "            model =  tf.keras.models.load_model(f.name)\n",
    "        model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=self.epochs, validation_split=0.1)\n",
    "        \n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"NNFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NNFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf1b5f-d6b9-4b89-be19-b3d22a5db0bd",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/NN_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d4468-2e0b-4110-9cad-c7ee2ce1a55c",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0c095-5a8c-4e3e-b205-f98649d0d4fa",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/local/NN_flow.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359072d7-d77e-465c-b110-adffe7c85445",
   "metadata": {},
   "source": [
    "## Lesson Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f190ead-e4c8-4108-9aeb-210aff6c4db7",
   "metadata": {},
   "source": [
    "In this lesson, we covered the following:\n",
    "\n",
    "* Understand what the Modern Data Stack for Production ML looks like\n",
    "* Appreciate where the focus of scientists needs to be in this stack\n",
    "* Write machine learning flows in Metaflow for \n",
    "    - Random forests\n",
    "    - Boosted trees\n",
    "    - Neural networks\n",
    "* Train models in parallel and choose most performant one using Metaflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2f123-47d1-430e-9b65-3cab5e803d06",
   "metadata": {},
   "source": [
    "In the next lesson, we'll take these machine learning workflows and see how to get them running on the cloud!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full-stack-metaflow] *",
   "language": "python",
   "name": "conda-env-full-stack-metaflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
