{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923b9a08-9b88-424d-9512-4fef84a51b4f",
   "metadata": {},
   "source": [
    "# Lesson 2: Machine Learning Workflows and DAGs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffacc8-f2c7-4a0c-9003-1fbdd3df2a89",
   "metadata": {},
   "source": [
    "## Learning objectives of this lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43465cf4-dc05-44b9-aab8-d95fc9ee644b",
   "metadata": {},
   "source": [
    "* Take ML from prototype to production\n",
    "* Appreciate where the focus of scientists needs to be in this stack\n",
    "* Write machine learning flows in Metaflow for \n",
    "    - Random forests\n",
    "    - Boosted trees\n",
    "    - Neural networks\n",
    "* Train models in parallel and choose most performant one using Metaflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab25a56-cc11-431d-8f5a-5c69c5f5a768",
   "metadata": {},
   "source": [
    "This lesson will focus on building local machine learning workflows using Metaflow, which allows data scientists to focus on the top layers of the ML stack, while having access to the infrastructural layers. \n",
    "\n",
    "For example, Metaflow helps with orchestration, and it integrates with other popular orchestrators like Argo, Step Functions and soon Airflow, so you don’t have to learn a separate tool just for orchestration.\n",
    "\n",
    "While there are many other tools just taking care of orchestration, like Airflow, they are not specifically meant for data scientists and require you to spend much more time thinking about infrastructure, like where and how to run the tasks and how to move data between tasks. Metaflow was created at Netflix specifically to serve the day-to-day needs of data scientists, so you don’t have to keep reinventing the wheel.\n",
    "\n",
    "Attendees will get a feel for writing flows and DAGs to define the steps in their workflows. We’ll also use DAG cards to visualize our ML workflows. This lesson will be local computation and in the next lesson, we’ll burst to the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611f465-a272-434b-8331-2158cf8e9870",
   "metadata": {},
   "source": [
    "In this section, we take the machine learning scripts from the previous lesson and turn them into flows. Currently, in the spirit of not introducing more tools, we'll write our flows in notebook cells and we'll execute them using the command line in Jupyter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f45945-20ee-4541-a8e7-15f4e9bc368f",
   "metadata": {},
   "source": [
    "## The Modern Data Stack for Production ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef009c-4e06-4078-8a69-1b175f7f6e5d",
   "metadata": {},
   "source": [
    "There are many layers to the modern ML stack. Consider a not atypical process of a data scientist moving their models from prototype to production:\n",
    "\n",
    "- You'll commonly use notebooks and .csv's for prototype → when moving to prod, you may need integrate with data warehouse or directly pull data from s3 every time you execute a model; or train on streaming data;\n",
    "- Then perhaps you'll need to store your model and artifacts in a data warehouse/lake and possibly so every time you run it;\n",
    "– In this case, we see the data/storage concern interacting with the model versioning question;\n",
    "- Then perhaps your data volume grows → you need a larger instance so we burst to the cloud; what happens to libraries? How do you migrate your state?\n",
    "- Then let’s say you need to train on GPUs but not for data processing: GPUs are expensive so we want to be able to send the training task there but not the rest → (data) workflows are a useful abstraction here.\n",
    "- and much more...\n",
    "\n",
    "\n",
    "So what does the modern stack look like? The following is from [Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure), published by Manning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dedc06-05f9-4ad5-bfb1-cd524664ff89",
   "metadata": {},
   "source": [
    "![flow0](../img/modern-ML-stack.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1bcc2-b599-48c3-91b8-f13b6606c743",
   "metadata": {},
   "source": [
    "Although they are all incredibly important, data scientists should NOT be expected to be experts at all of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9806d-3e05-46d4-9b76-b707cad73198",
   "metadata": {},
   "source": [
    "![flow0](../img/data-triangle.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293f9fe-4d23-4593-ad69-2c563216d279",
   "metadata": {},
   "source": [
    "Let's have a look at some of what data scientists do to interact with the infrastructural layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54a906-aeb2-4a96-b949-2b035a8faf50",
   "metadata": {},
   "source": [
    "![flow0](../img/ds-workflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba509ed-e64e-4940-bc11-f5b41a75c017",
   "metadata": {},
   "source": [
    "And let's not forget that this is an iterative process!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d09cc-addf-465c-85bf-881c0c1c82f8",
   "metadata": {},
   "source": [
    "![flow0](../img/data-science-iteration.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550b698-d7cd-426a-84e4-5ccce7466d72",
   "metadata": {},
   "source": [
    "## Orchestrating your Machine Learning workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa70b76-a718-4544-9ddb-62f977262242",
   "metadata": {},
   "source": [
    "The complexity of ML workflows can be become complicated very quickly. To avoid a jungle of data pipelines and models, we need organizational principles for our workflows and _directed acyclic graphs_ have become a standard for this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e7e33-cd2e-4834-9397-3d6fb03e9793",
   "metadata": {},
   "source": [
    "<img src=\"../img/flow_ex_0.png\" alt=\"DAG\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849a81d-20c8-4337-8050-b69f328cc9fe",
   "metadata": {},
   "source": [
    "DAGs can often be more complicated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a624d-6cd3-4dd7-b98a-d79439c9a7b7",
   "metadata": {},
   "source": [
    "![flow0](../img/flow_ex_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bfd6e-0ba1-48c4-a747-a92b837afd1a",
   "metadata": {},
   "source": [
    "## A Word on the Basics of Metaflow\n",
    "\n",
    "> Metaflow follows the dataflow paradigm which models a program as a directed graph of operations. This is a natural paradigm for expressing data processing pipelines, machine learning in particular. We call the graph of operations a flow. You define the operations, called steps, which are nodes of the graph and contain transitions to the next steps, which serve as edges.\n",
    "\n",
    "> Metaflow sets some constraints on the structure of the graph. For starters, every flow needs a step called start and a step called end. An execution of the flow, which we call a run, starts at start. The run is successful if the final end step finishes successfully. What happens between start and end is up to you.\n",
    "\n",
    "-- The [Metaflow docs](https://docs.metaflow.org/metaflow/basics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7685a-982d-41e9-91d2-1d947b958df7",
   "metadata": {},
   "source": [
    "## Crafting Metaflows for Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0ab17-f71d-4e35-8862-346db0b1bbdb",
   "metadata": {},
   "source": [
    "ML flows can be broken down into steps, such as:\n",
    "\n",
    "- importing data\n",
    "- processing, wrangling, and/or transforming the data\n",
    "- data validation\n",
    "- model configuration\n",
    "- model training, and\n",
    "- model deployment.\n",
    "\n",
    "The first flow we write will be a template showing these steps.\n",
    "\n",
    "As stated above, in the spirit of not introducing more tools, we'll write our flows in notebook cells and we'll execute them using the command line in Jupyter Lab. Having said that, we encourage everybody to get comfortable writing Python scripts and using the command line as much as possible! Notebooks are wonderful for many things but not necessarily for writing and executing ML workflows.\n",
    "\n",
    "\n",
    "At the end of the day, modern data science applications are real software projects. Applying software engineering principles like structuring code as functions and modules is useful and hence as you get up to speed, you might find a modern IDE like Visual Studio Code quite convenient in writing your workflows. Don’t worry, it is still the same code running in flows as what you would write in notebooks, so there’s not much new to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25950f2-13e6-4f33-a882-8f928b952b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../flows/local/flow_template.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Template for writing Metaflows\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from metaflow import FlowSpec, step, current, card\n",
    "\n",
    "\n",
    "class Template_Flow(FlowSpec):\n",
    "    \"\"\"\n",
    "    Template for Metaflows.\n",
    "    You can choose which steps suit your workflow.\n",
    "    We have included the following common steps:\n",
    "    - Start\n",
    "    - Process data\n",
    "    - Data validation\n",
    "    - Model configuration\n",
    "    - Model training\n",
    "    - Model deployment\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start Step for a Flow;\n",
    "        \"\"\"\n",
    "        print(\"flow name: %s\" % current.flow_name)\n",
    "        print(\"run id: %s\" % current.run_id)\n",
    "        print(\"username: %s\" % current.username)\n",
    "\n",
    "        # Call next step in DAG with self.next(...)\n",
    "        self.next(self.process_raw_data)\n",
    "\n",
    "    @step\n",
    "    def process_raw_data(self):\n",
    "        \"\"\"\n",
    "        Read and process data\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll read in and process your data\")\n",
    "\n",
    "        self.next(self.data_validation)\n",
    "\n",
    "    @step\n",
    "    def data_validation(self):\n",
    "        \"\"\"\n",
    "        Perform data validation\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll write your data validation code\")\n",
    "\n",
    "        self.next(self.get_model_config)\n",
    "\n",
    "    @step\n",
    "    def get_model_config(self):\n",
    "        \"\"\"\n",
    "        Configure model + hyperparams\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll configure your model + hyperparameters\")\n",
    "        self.next(self.train_model)\n",
    "\n",
    "    @step\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train your model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll train your model\")\n",
    "\n",
    "        self.next(self.deploy)\n",
    "\n",
    "    @step\n",
    "    def deploy(self):\n",
    "        \"\"\"\n",
    "        Deploy model\n",
    "        \"\"\"\n",
    "        print(\"In this step, you'll deploy your model\")\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        DAG is done! Congrats!\n",
    "        \"\"\"\n",
    "        print('DAG ended! Woohoo!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Template_Flow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed676e5-0600-4b88-a4c1-e4780793ca3a",
   "metadata": {},
   "source": [
    "You can now run this flow by executing the following from the CLI:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac9c19-486c-4ce8-93e1-baef6eab08db",
   "metadata": {},
   "source": [
    "We can also (cheekily) execute from this NB using the handy iPython magic shBang (!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff4783-4962-4422-8654-bb3d40abb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../flows/local/flow_template.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727715f2-98e2-461f-9841-e019c3a66dba",
   "metadata": {},
   "source": [
    "Success! But what are all these outputs? I'm glad that you asked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f0f3cb-4faf-452d-8a81-e457f01c286d",
   "metadata": {},
   "source": [
    "![flow0](../img/mf_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594071a-8dbf-4e42-b488-f7d53cb54152",
   "metadata": {},
   "source": [
    "- **Timestamp** denotes when the line was output.\n",
    "- The information inside the square brackets identifies a **task**.\n",
    "- Every Metaflow run gets a unique ID, a **run ID**.\n",
    "- A run executes the steps in order. The step that is currently being executed is denoted by **step name**.\n",
    "- A step may spawn multiple tasks which are identified by a **task ID**.\n",
    "- The combination of a flow name, run ID, step name, and a task ID,uniquely identify a task in your Metaflow environment, amongst all runs of any flows. Here, the flow name is omitted since it is the same for all lines. We call this globally unique identifier a **pathspec**.\n",
    "- Each task is executed by a separate process in your operating system, identified by a **process ID** aka _pid_. You can use any operating system level monitoring tools such as top to monitor resource consumption of a task based on its process ID.\n",
    "- After the square bracket comes a **log message** that may be a message output by Metaflow itself, like “Task is starting” in this example, or a line output by your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783703ae-daa1-4126-97cb-afcbc2511cd8",
   "metadata": {},
   "source": [
    "### Metaflow cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495bbe1-9619-4910-b693-365ebfb840e0",
   "metadata": {},
   "source": [
    "We can use MF cards to visualize aspects of our flow. In this case, there's not much to check out but we **can** see the DAG by using the CLI as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04417d-4786-405c-845c-28348f8afed1",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/local/flow_template.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b756922-edf6-44d9-8994-e4414ceb290a",
   "metadata": {},
   "source": [
    "## Time to Write a Metaflow Flow: Orchestrating our Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebaf4f-7e2c-4c1c-aa62-9a045c9275e7",
   "metadata": {},
   "source": [
    "In this section, we'll turn the random forest from the previous lesson into a flow. Recall our RF code from Lesson 1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f8503-5a93-4643-a378-347fd1637525",
   "metadata": {},
   "source": [
    "```\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores_rf = cross_val_score(clf_rf, X, y, cv=5)\n",
    "print(scores_rf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044535b1-c653-41e4-a2ef-94c51997a53b",
   "metadata": {},
   "source": [
    "In order to turn this into a (Metaflow) flow, you first need to decide what your steps are going to be. In this case, we’ll have distinct steps to \n",
    "\n",
    "* Load the data\n",
    "* Instantiate the model\n",
    "* Train the model and perform cross-validation\n",
    "\n",
    "In general, this involves some design choices on the user’s part and we have some general rules of thumb [here](https://docs.metaflow.org/metaflow/basics).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To then write your flow, you\n",
    "\n",
    "* Import `FlowSpec` and `step` outside your flow\n",
    "* Include step-specific imports within each step\n",
    "* Assign any data structures you wish to pass between steps to self\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b251d7-2c5e-4c40-9b95-205eddcde70e",
   "metadata": {},
   "source": [
    "## A Reminder on Why We're Doing This"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481ee6a-1c2f-49a3-912b-2b33104e4c03",
   "metadata": {},
   "source": [
    "As there is significant overhead in doing this work, let's be explicit as to the benifits:\n",
    "- Notebooks are great when you want to tell a concise linear narrative as a single article. That’s why they are so great for teaching and learning. However, real-world applications tend to be non-linear, so it is better to structure the project as an explicit DAG which makes it more understandable for humans and more easily executable by computers.\n",
    "- A superpower of DAGs is that if you have parallel branches in your DAG, computer can execute them in parallel automatically, making your project finish much faster without you having to change much anything.\n",
    "- This means that you can e.g. train 200 models - one for each country of the world in parallel, even using multiple computers - which would be impossible to do in a notebook.\n",
    "- It is much easier to take a workflow, which is a proper Python project, to production than a notebook, in particular when using a project like Metaflow that integrates with modern workflow schedulers and compute platforms like Kubernetes. This will make engineers happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a864d3-32ad-4841-a684-66a53c3c4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../flows/local/rf_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "class RF_Flow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a random forest\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model)\n",
    "        \n",
    "\n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"RF_Flow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    RF_Flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7703623-d4cb-4641-81f1-19ddb43c6fb0",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/rf_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53459547-9b72-4eec-944e-71db6517faa0",
   "metadata": {},
   "source": [
    "We can check out the Metaflow card:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643169f-154d-4cee-a855-1e02db08cf9c",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/local/rf_flow.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3503b3-a53d-4013-86be-e950ab54690e",
   "metadata": {},
   "source": [
    "## The Unreasonable Effective of Branching Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8d26a-99d2-48dc-8c41-f3e405e557e3",
   "metadata": {},
   "source": [
    "Often, you'll want to train several independent models and then choose the best one. As they're indepedendent, you can train them in parallel. For this, you can use the concept of branching, which is exemplified in this figure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e815da0-ee75-43c9-ba15-a4f25df7f133",
   "metadata": {},
   "source": [
    "<img src=\"../img/flow_ex_0.png\" alt=\"DAG\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde03f82-6d44-476c-99b2-1eb41e390d4f",
   "metadata": {},
   "source": [
    "We'll show how to do this in Metaflow by writing a flow that has random forests, decision trees, and extra trees classifiers, trains them all and chooses the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545fc2d-dcc4-4a37-bc8b-37ed28f98ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../flows/local/tree_branch_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "class Branch_Flow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train multiple tree based methods\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        #Import scikit-learn dataset library\n",
    "        from sklearn import datasets\n",
    "\n",
    "        #Load dataset\n",
    "        self.iris = datasets.load_iris()\n",
    "        self.X = self.iris['data']\n",
    "        self.y = self.iris['target']\n",
    "        self.next(self.rf_model, self.xt_model, self.dt_model)\n",
    "    \n",
    "                \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "        \"\"\"\n",
    "        build random forest model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def xt_model(self):\n",
    "        \"\"\"\n",
    "        build extra trees classifier\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "\n",
    "        self.clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "            min_samples_split=2, random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "        self.next(self.choose_model)\n",
    "\n",
    "    @step\n",
    "    def dt_model(self):\n",
    "        \"\"\"\n",
    "        build decision tree classifier\n",
    "        \"\"\"\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        self.clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "            random_state=0)\n",
    "\n",
    "        self.scores = cross_val_score(self.clf, self.X, self.y, cv=5)\n",
    "\n",
    "        self.next(self.choose_model)\n",
    "                        \n",
    "    @step\n",
    "    def choose_model(self, inputs):\n",
    "        \"\"\"\n",
    "        find 'best' model\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        def score(inp):\n",
    "            return inp.clf,\\\n",
    "                   np.mean(inp.scores)\n",
    "\n",
    "            \n",
    "        self.results = sorted(map(score, inputs), key=lambda x: -x[1]) \n",
    "        self.model = self.results[0][0]\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print('Scores:')\n",
    "        print('\\n'.join('%s %f' % res for res in self.results))\n",
    "        print('Best model:')\n",
    "        print(self.model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Branch_Flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fc354-9ef2-4326-a913-f2299fd3d8b4",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/tree_branch_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498134b-2f69-4dd2-b604-6e9c6e44fd49",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02c584-0fdb-459c-82b6-bbac836b2525",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/local/tree_branch_flow.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd4a43-3ea3-43f5-9d2f-c96ee7d18bb3",
   "metadata": {},
   "source": [
    "## Boosted Trees Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b27a2-f26c-471d-8164-598ed7d4cfb0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the XGBoost example from Lesson 1 into a flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b86aa-498d-474f-bde4-1e183b5dd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../flows/local/boosted_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "\n",
    "class BSTFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a boosted tree\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data & train model\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "        dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "        param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic','eval_metric':'logloss'}\n",
    "        num_round = 2\n",
    "        bst = xgb.train(param, dtrain, num_round)\n",
    "        bst.save_model(\"model.json\")\n",
    "        self.next(self.predict)\n",
    "        \n",
    "    @step\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        make predictions\n",
    "        \"\"\"\n",
    "        import xgboost as xgb\n",
    "\n",
    "        dtest = xgb.DMatrix('data/agaricus.txt.test')\n",
    "        # make prediction\n",
    "        bst = xgb.Booster()\n",
    "        bst.load_model(\"model.json\")\n",
    "        preds = bst.predict(dtest)\n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"BSTFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BSTFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1fce4-47cd-4c23-b770-7af10f785aee",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/boosted_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757332f-b475-4e42-b546-c97a75068fc9",
   "metadata": {},
   "source": [
    "## Your First Deep Learning Metaflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95df17-f187-4d4f-ad2e-894898f9b9b0",
   "metadata": {},
   "source": [
    "In this section, we'll turn the deep learning example above into a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adeacf-5396-42d0-bed2-32a8a9763d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../flows/local/NN_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, JSONType, IncludeFile, card\n",
    "from taxi_modules import init, MODELS, MODEL_LIBRARIES\n",
    "import json\n",
    "\n",
    "\n",
    "class NNFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train a NN\n",
    "    \"\"\"\n",
    "    @card\n",
    "    @step\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Load the data\n",
    "        \"\"\"\n",
    "        from tensorflow import keras\n",
    "\n",
    "        # the data, split between train and test sets\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.mnist.load_data()\n",
    "        self.next(self.wrangle)\n",
    "        \n",
    "    @step\n",
    "    def wrangle(self):\n",
    "        \"\"\"\n",
    "        massage data\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from tensorflow import keras\n",
    "        # Model / data parameters\n",
    "        self.num_classes = 10\n",
    "        self.input_shape = (28, 28, 1)\n",
    "\n",
    "        # Scale images to the [0, 1] range\n",
    "        self.x_train = self.x_train.astype(\"float32\") / 255\n",
    "        self.x_test = self.x_test.astype(\"float32\") / 255\n",
    "        # Make sure images have shape (28, 28, 1)\n",
    "        self.x_train = np.expand_dims(self.x_train, -1)\n",
    "        self.x_test = np.expand_dims(self.x_test, -1)\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        self.y_train = keras.utils.to_categorical(self.y_train, self.num_classes)\n",
    "        self.y_test = keras.utils.to_categorical(self.y_test, self.num_classes)\n",
    "        \n",
    "        self.next(self.build_model)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        build NN model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=self.input_shape),\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(self.num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            tf.keras.models.save_model(model, f.name, save_format='h5')\n",
    "            self.model = f.read()\n",
    "        self.next(self.train)\n",
    "\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import tensorflow as tf\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 15\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            f.write(self.model)\n",
    "            f.flush()\n",
    "            model =  tf.keras.models.load_model(f.name)\n",
    "        model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=self.epochs, validation_split=0.1)\n",
    "        \n",
    "        self.next(self.end)\n",
    "        \n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        End of flow!\n",
    "        \"\"\"\n",
    "        print(\"NNFlow is all done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NNFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf1b5f-d6b9-4b89-be19-b3d22a5db0bd",
   "metadata": {},
   "source": [
    "Execute the above from the command line with\n",
    "\n",
    "```bash\n",
    "python flows/local/NN_flow.py run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d4468-2e0b-4110-9cad-c7ee2ce1a55c",
   "metadata": {},
   "source": [
    "We can also view the Metaflow card:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0c095-5a8c-4e3e-b205-f98649d0d4fa",
   "metadata": {},
   "source": [
    "```\n",
    "python flows/local/NN_flow.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359072d7-d77e-465c-b110-adffe7c85445",
   "metadata": {},
   "source": [
    "## Lesson Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f190ead-e4c8-4108-9aeb-210aff6c4db7",
   "metadata": {},
   "source": [
    "In this lesson, we covered the following:\n",
    "\n",
    "* Understand what the Modern Data Stack for Production ML looks like\n",
    "* Appreciate where the focus of scientists needs to be in this stack\n",
    "* Write machine learning flows in Metaflow for \n",
    "    - Random forests\n",
    "    - Boosted trees\n",
    "    - Neural networks\n",
    "* Train models in parallel and choose most performant one using Metaflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2f123-47d1-430e-9b65-3cab5e803d06",
   "metadata": {},
   "source": [
    "In the next lesson, we'll take these machine learning workflows and see how to get them running on the cloud!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:full-stack-metaflow] *",
   "language": "python",
   "name": "conda-env-full-stack-metaflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
